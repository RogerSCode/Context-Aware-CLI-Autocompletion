%%% Die folgende Zeile nicht Ã¤ndern!
\section*{\ifthenelse{\equal{\sprache}{englisch}}{Abstract}}
%%% Zusammenfassung:
We try to create an LLM based autocompletion system that uses context to create more accurate predictions. Our approach is to add context-information to the prompt. Therefore, we create a prompt-template that tells the model that it is an autocompletion function for a Linux terminal. Several variations of this are tested, and the best one is chosen. The prompt-template is populated during use with the path, the command is executed in and the names of the files in the current directory.



In search of a model to use, we have the criteria that it has to be transformer based and its tokenizer has to be of at least subword granularity. We also consider the tradeoff between accuracy and resource demand. We reject BERT for its architecture. GPT-3 and GPT-4 are considered but rejected because they are not free to use.
So we decide on their predecessor, GPT-2 and the LLaMA based Alpaca that is fine-tuned to behave like GPT-3



Our tests show that with just the command, the models do not generate useful results. The usage of our prompt-template show an improvement. But the directory and files have little to no benefit, even tough they clearly influence the predictions. Which leads us to the conclusion that the base models are not good enough for this task.

Finally, we discuss methods to improve this, ranging from further prompt modifications over the use of different models to fine-tuning them. Also, the possible usage of methods to limit the requirements of computational resources are discussed.