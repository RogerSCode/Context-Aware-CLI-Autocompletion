%%% Die folgende Zeile nicht Ã¤ndern!
\section*{\ifthenelse{\equal{\sprache}{englisch}}{Abstract}}
%%% Zusammenfassung:



Stochastic Autocompletion systems for the terminal are nothing new, but the recent development of large-language-models(LLMs) allows for new approaches. These LLMs integrate world knowledge, leading to a potential spillover effect that can enhance autocompletion systems.



While the completion of CLI (Command Line Interface) commands is possible, a more intriguing question is whether using the path of command execution and the files contained in the current directory as additional context can improve the accuracy of completion predictions.


For  example, it is way more likely to use a git command in a Git repository than in a downloads folder. Furthermore, a command beginning with 'git commit' is more likely to be succeeded by 'git push' than 'git pull'.



The specific prompt can hugely influence the accuracy of the model's predictions.

For this reason, we try to find a prompt-template that improves the accuracy.

Which consists of telling the model what it which is an autocompletion and an order what we want the model to do which is to predict the CLI command. Additionally, we add the variable context of the current directory and names of the files in the current directory. 



Next there is the issue of computational resources. With access to a sufficient cloud infrastructure, even high-end LLMs can be deployed. But this only applies if network access is available. Also, to a lesser extent, network speed can be an issue. 

Therefore, we should consider a locale solution. This  again would limit our available 

resources. Therefore, we have to make a trade-of between accuracy and size of the model.

